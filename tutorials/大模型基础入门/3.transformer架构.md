Transformer 架构教程（面向新手）
引言
Transformer 是一种革命性的神经网络架构，首次在 2017 年由 Google 研究人员在论文《Attention is All You Need》中提出。它最初被设计用于机器翻译任务，但由于其卓越的性能和灵活性，迅速成为自然语言处理（NLP）领域的标准模型，并扩展到计算机视觉、语音识别等其他领域。
Transformer 的关键创新在于其自注意力（self-attention）机制，这允许模型在处理序列数据时关注输入序列中的不同部分，从而捕捉长距离依赖关系，而无需使用循环或卷积操作。这使得 Transformer 能够高效地处理长序列数据，并在并行计算中表现出色。
本教程旨在为初学者提供一个全面而专业的 Transformer 架构介绍。我们将逐步讲解 Transformer 的各个组成部分，从输入处理到输出生成，并结合实际例子和图文说明，帮助您深入理解这一强大的模型。相比于现有的网页内容，本教程将更加详细、综合，并包含更多图文并茂的解释和案例。
高级概览
Transformer 架构由两个主要部分组成：编码器（Encoder）和解码器（Decoder）。在原始的 Transformer 模型中，编码器和解码器各包含多个相同的层（通常是 6 层），每层由两个子层组成：多头自注意力（Multi-Head Self-Attention）和前馈神经网络（Feed-Forward Neural Network）。此外，Transformer 还使用了残差连接（Residual Connections）和层归一化（Layer Normalization）来稳定训练过程。
对于本教程，我们将重点关注解码器-仅 Transformer（如 GPT），因为它在许多现代语言模型中被广泛使用。以下是 Transformer 架构的主要步骤：

分词（Tokenization）
词嵌入（Word Embeddings）
位置编码（Positional Encoding）
Transformer 块（Transformer Block）
残差连接和层归一化（Residual Connection and Layer Normalization）
前馈神经网络（Feed-Forward Network）
重复 Transformer 块
输出概率（Output Probabilities）

步骤 1：分词（Tokenization）
分词是将输入文本转换为模型可以处理的数字表示的过程。在 Transformer 中，输入文本首先被分割成一系列的 token，每个 token 对应一个单词或子词。然后，每个 token 被映射到一个独特的整数 ID。
示例：对于句子 "Hello, world!"，分词器可能会将其分解为 ["Hello", ",", "world", "!"]，然后每个 token 被映射到其对应的 ID，例如 [100, 101, 102, 103]。
代码示例：以下是一个使用 Python 和 tiktoken 库进行分词的简单示例：
import tiktoken

text = "Hello, world!"
tokenizer = tiktoken.get_encoding("gpt2")
tokens = tokenizer.encode(text)
print(tokens)  # 输出: [100, 101, 102, 103]

可视化：参考 The Illustrated Transformer (中文翻译) 中的分词示例图，展示文本如何被分割为 token。
补充说明：分词是 NLP 中非常重要的一步，不同的语言和任务可能需要不同的分词策略。例如，在中文中，由于没有明显的单词边界，分词通常需要更复杂的算法，如基于规则的分词或基于机器学习的分词。常用的中文分词工具包括 jieba 和 THULAC。
步骤 2：词嵌入（Word Embeddings）
词嵌入是将每个 token 的 ID 转换为密集的向量表示。这些向量捕捉了 token 在语义上的含义，并作为 Transformer 模型的输入。
在 Transformer 中，词嵌入通常是通过查找表（Look-Up Table）实现的，其中每个 token ID 对应一个固定大小的向量（例如，维度为 512）。
示例：假设我们有一个词汇表大小为 100,000 的模型，每个 token ID 都被映射到一个 512 维的向量。例如，token ID 100 的嵌入可能是一个 512 维的向量，如 [0.1, 0.2, ..., 0.5]。
表格：词嵌入查找表示例



Token ID
词嵌入向量（部分维度）



100
[0.1, 0.2, 0.3, ...]


101
[0.4, 0.5, 0.6, ...]


102
[0.7, 0.8, 0.9, ...]


代码示例：以下是一个使用 PyTorch 实现词嵌入的简单示例：
import torch
import torch.nn as nn

vocab_size = 100000
d_model = 512
embedding = nn.Embedding(vocab_size, d_model)
token_ids = torch.tensor([100, 101, 102])
embedded = embedding(token_ids)
print(embedded.shape)  # 输出: torch.Size([3, 512])

补充说明：词嵌入是 Transformer 中最基础的表示方式之一，它将离散的 token 转换为连续的向量空间。不同的预训练模型（如 Word2Vec、GloVe、BERT）可能使用不同的词嵌入方式，但核心思想都是将单词映射到一个低维空间中。
步骤 3：位置编码（Positional Encoding）
由于 Transformer 架构中没有循环或卷积操作，因此模型无法直接捕捉输入序列中的位置信息。为了解决这个问题，位置编码被添加到词嵌入中，以提供每个 token 在序列中的相对或绝对位置信息。
位置编码通常是通过正弦和余弦函数生成的，这样可以确保位置信息是可学习的，并且可以与词嵌入相加。
公式：对于位置 (i) 和维度 (j)，位置编码定义为：

(PE(i, 2j) = \sin(i / 10000^{2j/d}))
(PE(i, 2j+1) = \cos(i / 10000^{2j/d}))

其中 (d) 是嵌入维度（例如 512），(j) 是维度的索引。
示例：对于一个长度为 10 的序列，位置编码可以生成一个 (10 \times 512) 的矩阵，添加到词嵌入中。
代码示例：以下是一个使用 PyTorch 实现位置编码的示例：
import torch
import math

class PositionalEncoding(nn.Module):
    def __init__(self, d_model, max_len=5000):
        super(PositionalEncoding, self).__init__()
        pe = torch.zeros(max_len, d_model)
        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)
        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))
        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)
        pe = pe.unsqueeze(0)
        self.register_buffer('pe', pe)

    def forward(self, x):
        return x + self.pe[:, :x.size(1)]

d_model = 512
pos_encoder = PositionalEncoding(d_model)
x = torch.zeros(1, 10, d_model)  # 假设输入序列长度为 10
encoded = pos_encoder(x)
print(encoded.shape)  # 输出: torch.Size([1, 10, 512])

可视化：参考 The Illustrated Transformer (中文翻译) 中的位置编码图，展示正弦和余弦函数的波形。
补充说明：位置编码是 Transformer 的一个关键创新点，它解决了传统 RNN 和 CNN 在处理长序列时无法有效捕捉位置信息的问题。使用正弦和余弦函数生成的位置编码具有可学习性，并且可以与词嵌入进行线性组合。
步骤 4：Transformer 块（Transformer Block）
Transformer 块是 Transformer 架构的核心组件，每个块包含多头自注意力和前馈神经网络两个子层。
4.1 多头自注意力概览（Multi-Head Attention Overview）
多头自注意力允许模型同时关注输入序列的不同部分。它通过创建多个注意力“头”，每个头可以关注不同的信息方面，然后将这些注意力表示组合起来。
示例：假设我们有一个长度为 10 的序列，使用 8 个注意力头，每个头可以关注序列中的不同部分（例如，第一个头关注句子的主语，第二个头关注动词等）。
可视化：参考 The Illustrated Transformer (中文翻译) 中的多头注意力图，展示多个注意力头的并行计算。
4.2 准备 Q、K、V（Prepare Q, K, V）
在自注意力机制中，每个 token 被转换为三个向量：查询（Query, Q）、键（Key, K）和值（Value, V）。这些向量是通过线性变换从输入嵌入中获得的。
示例：对于一个输入序列长度为 10，嵌入维度为 512，Q、K、V 的维度分别为 (10 \times 512)。
代码示例：以下是一个使用 PyTorch 实现 Q、K、V 计算的示例：
import torch
import torch.nn as nn

d_model = 512
num_heads = 8
d_k = d_model // num_heads  # 每个头的维度为 64

q_linear = nn.Linear(d_model, d_model)
k_linear = nn.Linear(d_model, d_model)
v_linear = nn.Linear(d_model, d_model)

x = torch.rand(1, 10, d_model)  # 假设输入序列长度为 10
q = q_linear(x)  # [1, 10, 512]
k = k_linear(x)  # [1, 10, 512]
v = v_linear(x)  # [1, 10, 512]

4.3 计算 QK 注意力（Calculate QK Attention）
计算 Q 和 K 的点积，以获得注意力分数。这表示每个查询与每个键之间的相似度。
示例：对于一个查询 (Q_i) 和所有键 (K_j)，注意力分数为 (score = Q_i \cdot K_j^T)。
4.4 缩放（Scale）
由于点积可能导致数值不稳定，因此注意力分数被缩放，除以平方根的 (d_k)（每个注意力头的维度）。
示例：如果每个注意力头的维度为 64，则缩放因子为 (\sqrt{64} = 8)。
4.5 掩码（Mask）
在某些情况下（如解码器自注意力），需要掩码未来位置的 token，以防止模型在生成时看到未来的信息。
示例：在生成第 (t) 个 token 时，模型只能看到第 (t) 个 token 之前的 token。
可视化：参考 The Illustrated Transformer (中文翻译) 中的掩码矩阵图。
代码示例：以下是一个实现掩码的 PyTorch 示例：
def create_mask(size):
    mask = torch.triu(torch.ones(size, size), diagonal=1).bool()
    return mask

seq_len = 10
mask = create_mask(seq_len)
print(mask)

4.6 Softmax
将缩放后的注意力分数通过 softmax 函数，转换为概率分布，表示每个查询对每个键的关注程度。
示例：一个注意力分数矩阵可能为：[\begin{bmatrix}0.2 & 0.3 & 0.5 \0.1 & 0.4 & 0.5 \\end{bmatrix}]经过 softmax 后成为概率分布。
4.7 计算 V 注意力（Calculate V Attention）
使用 softmax 得到的注意力权重与 V 相乘，获得每个查询的加权值表示。
示例：如果注意力权重为 ([0.2, 0.3, 0.5])，V 为 ([V_1, V_2, V_3])，则输出为 (0.2 \cdot V_1 + 0.3 \cdot V_2 + 0.5 \cdot V_3)。
4.8 拼接和输出（Concatenate and Output）
将所有注意力头的输出拼接在一起，并通过一个线性层和残差连接，得到最终的注意力输出。
代码示例：以下是一个完整的多头注意力实现：
class MultiHeadAttention(nn.Module):
    def __init__(self, d_model, num_heads):
        super(MultiHeadAttention, self).__init__()
        self.d_model = d_model
        self.num_heads = num_heads
        self.d_k = d_model // num_heads
        
        self.q_linear = nn.Linear(d_model, d_model)
        self.k_linear = nn.Linear(d_model, d_model)
        self.v_linear = nn.Linear(d_model, d_model)
        self.out_linear = nn.Linear(d_model, d_model)
        
    def forward(self, q, k, v, mask=None):
        batch_size = q.size(0)
        
        q = self.q_linear(q).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)
        k = self.k_linear(k).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)
        v = self.v_linear(v).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)
        
        scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(self.d_k)
        if mask is not None:
            scores = scores.masked_fill(mask == True, -1e9)
        attention = torch.softmax(scores, dim=-1)
        context = torch.matmul(attention, v)
        
        context = context.transpose(1, 2).contiguous().view(batch_size, -1, self.d_model)
        output = self.out_linear(context)
        return output

mha = MultiHeadAttention(d_model=512, num_heads=8)
x = torch.rand(1, 10, 512)
output = mha(x, x, x)
print(output.shape)  # 输出: torch.Size([1, 10, 512])

步骤 5：残差连接和层归一化（Residual Connection and Layer Normalization）
残差连接允许梯度在深层网络中更容易地流动，而层归一化帮助稳定化训练过程，并加速收敛。
示例：如果输入为 (x)，Transformer 块的输出为 (y)，则残差连接后的输出为 (x + y)，然后应用层归一化。
可视化：参考 The Illustrated Transformer (中文翻译) 中的残差连接图。
代码示例：以下是一个实现残差连接和层归一化的示例：
class TransformerBlock(nn.Module):
    def __init__(self, d_model, num_heads):
        super(TransformerBlock, self).__init__()
        self.mha = MultiHeadAttention(d_model, num_heads)
        self.norm1 = nn.LayerNorm(d_model)
        self.ffn = nn.Sequential(
            nn.Linear(d_model, d_model * 4),
            nn.ReLU(),
            nn.Linear(d_model * 4, d_model)
        )
        self.norm2 = nn.LayerNorm(d_model)
        
    def forward(self, x, mask=None):
        attn_output = self.mha(x, x, x, mask)
        x = self.norm1(x + attn_output)
        ffn_output = self.ffn(x)
        x = self.norm2(x + ffn_output)
        return x

block = TransformerBlock(d_model=512, num_heads=8)
x = torch.rand(1, 10, 512)
output = block(x)
print(output.shape)  # 输出: torch.Size([1, 10, 512])

步骤 6：前馈神经网络（Feed-Forward Network）
每个 Transformer 块中的前馈神经网络是对每个位置的表示独立应用的全连接神经网络。它通常包含两个线性变换和一个非线性激活函数（如 ReLU）。
示例：对于一个输入向量 (x)，前馈网络的计算为：[\text{FFN}(x) = \text{ReLU}(W_1x + b_1)W_2 + b_2]
步骤 7：重复步骤 4-6（Repeat Steps 4-6）
Transformer 模型包含多个 Transformer 块（例如，6 个），每个块都应用上述步骤。
示例：一个 6 层的 Transformer 模型会将输入序列通过 6 个 Transformer 块依次处理。
代码示例：以下是一个多层 Transformer 的实现：
class Transformer(nn.Module):
    def __init__(self, d_model, num_heads, num_layers):
        super(Transformer, self).__init__()
        self.blocks = nn.ModuleList([
            TransformerBlock(d_model, num_heads) for _ in range(num_layers)
        ])
        
    def forward(self, x, mask=None):
        for block in self.blocks:
            x = block(x, mask)
        return x

transformer = Transformer(d_model=512, num_heads=8, num_layers=6)
x = torch.rand(1, 10, 512)
output = transformer(x)
print(output.shape)  # 输出: torch.Size([1, 10, 512])

步骤 8：输出概率（Output Probabilities）
最后，通过一个线性层和 softmax 函数，将 Transformer 块的输出转换为每个 token 的概率分布，表示下一个 token 的预测。
示例：对于一个词汇表大小为 10,000 的模型，输出是一个 (1 \times 10,000) 的概率分布，表示下一个 token 的可能性。
代码示例：以下是一个实现输出概率的示例：
class LanguageModel(nn.Module):
    def __init__(self, vocab_size, d_model, num_heads, num_layers):
        super(LanguageModel, self).__init__()
        self.embedding = nn.Embedding(vocab_size, d_model)
        self.pos_encoder = PositionalEncoding(d_model)
        self.transformer = Transformer(d_model, num_heads, num_layers)
        self.fc = nn.Linear(d_model, vocab_size)
        
    def forward(self, x, mask=None):
        x = self.embedding(x)
        x = self.pos_encoder(x)
        x = self.transformer(x, mask)
        x = self.fc(x)
        return x

model = LanguageModel(vocab_size=10000, d_model=512, num_heads=8, num_layers=6)
x = torch.randint(0, 10000, (1, 10))
output = model(x)
print(output.shape)  # 输出: torch.Size([1, 10, 10000])

可视化：参考 The Illustrated Transformer (中文翻译) 中的输出概率图。
结论
Transformer 架构通过其自注意力机制和并行计算能力，显著提高了序列到序列任务的性能。它已经成为现代 AI 模型的基石，特别是在自然语言处理领域。
通过本教程，我们希望您能够对 Transformer 的工作原理有更深入的理解，并为进一步学习和应用这一强大的模型打下基础。
参考文献

The Illustrated Transformer (中文翻译)
The Illustrated Transformer (中文翻译)
Attention is All You Need
Tensor2Tensor GitHub
Harvard NLP PyTorch Guide

