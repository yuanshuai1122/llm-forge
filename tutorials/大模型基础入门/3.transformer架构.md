# Transformer 架构教程

## 什么是 Transformer？

Transformer 是一种神经网络架构，最初由 Google 研究人员在 2017 年论文《[Attention is All You Need](https://arxiv.org/abs/1706.03762)》中提出，用于机器翻译。它通过自注意力机制（self-attention）处理序列数据，允许模型关注输入序列的不同部分，从而捕捉长距离依赖关系。与传统 RNN 不同，Transformer 支持并行计算，显著提高效率。

本教程将重点讲解解码器-仅 Transformer（如 GPT），因为它在语言建模任务中广泛应用。我们将通过一个简单句子示例（“我爱深度学习”）逐步展示每个步骤的工作原理，确保初学者能够轻松理解。

## 引言

Transformer 架构自 2017 年提出以来，彻底改变了自然语言处理（NLP）领域。它通过自注意力机制高效捕捉序列中的长距离依赖，取代了传统的 RNN 和 CNN。研究表明，Transformer 的并行计算能力使其在机器翻译、文本生成等任务中表现卓越，成为 GPT、BERT 等模型的核心。

## 高级概览

Transformer 由编码器（Encoder）和解码器（Decoder）组成，每部分包含多个层（通常 6 层），每层有：

- **多头自注意力**：关注序列不同部分，捕捉多种关系。
- **前馈神经网络**：对每个位置独立处理，增加非线性。
- **残差连接和层归一化**：稳定训练，改善梯度流动。

<img src="../images/decoder-only-transformer.jpg" alt="decoder-only-transformer" style="zoom: 25%;" />

本教程聚焦解码器-仅 Transformer，适用于语言建模。以下是主要步骤：

| 步骤                   | 描述                             | 作用                               |
| ---------------------- | -------------------------------- | ---------------------------------- |
| 1. 分词                | 将文本分割为 token 并映射为 ID。 | 将文本转换为模型可处理的数字表示。 |
| 2. 词嵌入              | 将 token ID 转为密集向量。       | 捕捉 token 的语义信息。            |
| 3. 位置编码            | 添加序列位置信息。               | 提供 token 顺序信息。              |
| 4. Transformer 块      | 包含多头自注意力和前馈网络。     | 捕捉序列依赖并处理表示。           |
| 5. 残差连接和层归一化  | 稳定训练。                       | 改善梯度流动和训练稳定性。         |
| 6. 前馈神经网络        | 独立处理每个位置。               | 增加非线性，增强表示能力。         |
| 7. 重复 Transformer 块 | 堆叠多层。                       | 学习复杂模式。                     |
| 8. 输出概率            | 生成下一个 token 的概率。        | 预测输出序列。                     |

## 步骤 1：分词（Tokenization）

### 什么是分词？

分词是将输入文本分解为较小单元（token）的过程，这些单元可以是单词、子词或字符。每个 token 被映射到一个唯一的整数 ID，作为模型的输入。

### 为什么需要分词？

神经网络无法直接处理文本，需要将文本转换为数字表示。分词将文本分解为离散单元，允许模型通过嵌入层将这些单元转换为向量。此外，分词处理未知词汇（OOV），通过子词分词（如 BPE）将未知词分解为常见子词。

### 如何在 Transformer 中实现分词？

Transformer 通常使用子词分词技术，如 Byte Pair Encoding（BPE）或 WordPiece。这些方法通过统计分析构建词汇表，平衡词汇表大小和覆盖率。例如，BPE 从字符开始，逐步合并高频字符对，形成子词。

**示例：**
对于句子“我爱深度学习”，分词器可能将其分解为 ["我", "爱", "深度", "学习"]，映射为 ID [100, 101, 102, 103]。若“深度学习”不在词汇表中，可能分解为 ["深", "度", "学", "习"]。

**代码示例：**

```python
import tiktoken

text = "我爱深度学习"
tokenizer = tiktoken.get_encoding("gpt2")
tokens = tokenizer.encode(text)
print(tokens)  # 示例输出: [100, 101, 102, 103]
```

**可视化：**
参考《[The Illustrated Transformer](https://jalammar.github.io/illustrated-transformer/)》中的分词图，展示文本如何被分割为 token。

**补充说明：**
中文分词因缺乏明显词界而需复杂算法，常用工具如 `jieba`。子词分词在多语言模型中更常见，因其能处理多种语言的未知词汇。

## 步骤 2：词嵌入（Word Embeddings）

### 什么是词嵌入？

词嵌入是将 token ID 转换为密集向量表示的过程。这些向量捕捉 token 的语义信息，作为 Transformer 的输入。

### 为什么需要词嵌入？

词嵌入将离散的 token 转换为连续的向量空间，使模型能够学习词汇之间的语义关系。例如，“猫”和“狗”的嵌入向量可能更接近，而与“桌子”较远。这种表示有助于模型理解上下文和泛化。

### 如何在 Transformer 中实现词嵌入？

通过查找表（Look-Up Table），每个 token ID 对应一个固定维度向量（例如 512 维）。嵌入矩阵的形状为 (词汇表大小, 嵌入维度)，如 (100,000, 512)。

**示例：**
对于“我爱深度学习”的 token ID [100, 101, 102, 103]，假设嵌入维度为 512，嵌入矩阵将生成形状为 (4, 512) 的向量表示。

**表格：词嵌入示例**

| Token ID     | 词嵌入向量（部分维度） |
| ------------ | ---------------------- |
| 100 ("我")   | [0.1, 0.2, 0.3, ...]   |
| 101 ("爱")   | [0.4, 0.5, 0.6, ...]   |
| 102 ("深度") | [0.7, 0.8, 0.9, ...]   |
| 103 ("学习") | [0.2, 0.3, 0.4, ...]   |

**代码示例：**

```python
import torch
import torch.nn as nn

vocab_size = 100000
d_model = 512
embedding = nn.Embedding(vocab_size, d_model)
token_ids = torch.tensor([100, 101, 102, 103])
embedded = embedding(token_ids)
print(embedded.shape)  # 输出: torch.Size([4, 512])
```

**补充说明：**
词嵌入通常通过训练学习，初始为随机值。预训练模型（如 BERT）可能使用上下文嵌入，但解码器-仅 Transformer 使用静态嵌入。

## 步骤 3：位置编码（Positional Encoding）

### 什么是位置编码？

位置编码是为每个 token 添加位置信息的机制，因为 Transformer 没有循环或卷积操作来捕捉 token 顺序。

### 为什么需要位置编码？

自注意力机制是置换不变的，不考虑 token 顺序。例如，没有位置编码，模型无法区分“我爱深度学习”和“学习深度爱我”。位置编码通过为每个位置生成唯一向量，告知模型 token 的相对或绝对位置。

### 如何在 Transformer 中实现位置编码？

使用正弦和余弦函数生成位置编码，公式为：

- ( PE(pos, 2i) = \sin(pos / 10000^{2i / d_{model}}) )
- ( PE(pos, 2i+1) = \cos(pos / 10000^{2i / d_{model}}) )

其中 ( pos ) 是位置，( i ) 是维度索引，( d_{model} ) 是嵌入维度（如 512）。这些函数生成周期性信号，低维度捕捉高频（局部）信息，高维度捕捉低频（全局）信息。

**为什么使用正弦和余弦？**
正弦和余弦函数是周期性的，允许模型学习相对位置关系，且对未见过的序列长度具有泛化能力。它们还确保编码值在 [-1, 1] 范围内，与词嵌入兼容。

**示例：**
对于“我爱深度学习”（4 个 token），位置编码生成形状为 (4, 512) 的矩阵，添加到词嵌入中。假设 ( d_{model} = 4 )，位置 0 和 1 的编码如下：

| 位置 | 维度 0                 | 维度 1                 | 维度 2                    | 维度 3               |
| ---- | ---------------------- | ---------------------- | ------------------------- | -------------------- |
| 0    | sin(0/10000^0) ≈ 0     | cos(0/10000^0) ≈ 1     | sin(0/10000^2/4) ≈ 0      | cos(0/10000^2/4) ≈ 1 |
| 1    | sin(1/10000^0) ≈ 0.841 | cos(1/10000^0) ≈ 0.540 | sin(1/10000^2/4) ≈ 0.0001 | cos(1/10000^2/4) ≈ 1 |

**代码示例：**

```python
import torch
import math

class PositionalEncoding(nn.Module):
    def __init__(self, d_model, max_len=5000):
        super().__init__()
        pe = torch.zeros(max_len, d_model)
        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)
        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))
        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)
        pe = pe.unsqueeze(0)
        self.register_buffer('pe', pe)

    def forward(self, x):
        return x + self.pe[:, :x.size(1)]
```

**可视化：**
参考《[The Illustrated Transformer](https://jalammar.github.io/illustrated-transformer/)》中的位置编码图，展示正弦和余弦波形如何随位置变化。

## 步骤 4：Transformer 块（Transformer Block）

### 什么是 Transformer 块？

Transformer 块是 Transformer 架构的核心，包含多头自注意力和前馈神经网络两个子层，每个子层后接残差连接和层归一化。

### 为什么需要 Transformer 块？

Transformer 块通过自注意力捕捉序列中的依赖关系，通过前馈网络增加非线性，允许模型学习复杂模式。残差连接和层归一化确保深层网络的训练稳定性。

#### 4.1 多头自注意力概览

**什么是自注意力？**
自注意力是一种机制，允许每个 token 关注序列中的所有其他 token，计算它们之间的相关性。它通过查询（Query）、键（Key）和值（Value）向量实现。

**什么是多头自注意力？**
多头自注意力将自注意力分成多个“头”，每个头关注序列的不同方面（如语法、语义），然后将结果拼接起来。

**为什么使用多头自注意力？**
多头机制增加模型容量，允许同时学习多种关系。例如，一个头可能关注主语-动词关系，另一个关注修饰语。

**数值示例：**
对于“我爱深度学习”（4 个 token），假设 ( d_{model} = 4 )，单头注意力，( d_k = d_v = 4 )。假设嵌入向量为：

- “我”: [1, 0, 0, 0]
- “爱”: [0, 1, 0, 0]
- “深度”: [0, 0, 1, 0]
- “学习”: [0, 0, 0, 1]

通过线性变换生成 Q、K、V（简化假设 Q = K = V = 嵌入向量）：

- 分数：( \text{score} = \frac{QK^T}{\sqrt{d_k}} )，( d_k = 4 )
- 对于“我”：( Q_1 \cdot K^T = [1, 0, 0, 0] )，分数 = [1, 0, 0, 0] / 2 = [0.5, 0, 0, 0]
- Softmax：[0.576, 0.141, 0.141, 0.141]
- 输出：0.576*[1,0,0,0] + 0.141*[0,1,0,0] + 0.141*[0,0,1,0] + 0.141*[0,0,0,1] ≈ [0.576, 0.141, 0.141, 0.141]

**可视化：**
参考《[The Illustrated Transformer](https://jalammar.github.io/illustrated-transformer/)》中的注意力图，展示注意力权重如何连接不同 token。

#### 4.2 准备 Q、K、V

通过线性变换从输入嵌入生成 Q、K、V 向量，矩阵形状为 ( (d_{model}, d_{model}) )。

#### 4.3 计算 QK 注意力

计算 Q 和 K 的点积，得到注意力分数。

#### 4.4 缩放

分数除以 (\sqrt{d_k})，防止大值导致 softmax 饱和。

#### 4.5 掩码

在解码器中，使用因果掩码（causal mask）防止模型关注未来 token。例如，对于“我爱深度学习”，生成“学习”时只能看到“我爱深度”。

**代码示例：**

```python
def create_mask(size):
    mask = torch.triu(torch.ones(size, size), diagonal=1).bool()
    return mask
```

#### 4.6 Softmax

将分数通过 softmax 转换为概率分布。

#### 4.7 计算 V 注意力

使用注意力权重加权 V，生成上下文向量。

#### 4.8 拼接和输出

拼接多头输出，通过线性层生成最终注意力输出。

**代码示例：**

```python
class MultiHeadAttention(nn.Module):
    def __init__(self, d_model, num_heads):
        super().__init__()
        self.d_model = d_model
        self.num_heads = num_heads
        self.d_k = d_model // num_heads
        self.q_linear = nn.Linear(d_model, d_model)
        self.k_linear = nn.Linear(d_model, d_model)
        self.v_linear = nn.Linear(d_model, d_model)
        self.out_linear = nn.Linear(d_model, d_model)

    def forward(self, q, k, v, mask=None):
        batch_size = q.size(0)
        q = self.q_linear(q).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)
        k = self.k_linear(k).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)
        v = self.v_linear(v).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)
        scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(self.d_k)
        if mask is not None:
            scores = scores.masked_fill(mask == 0, -1e9)
        attention = torch.softmax(scores, dim=-1)
        context = torch.matmul(attention, v)
        context = context.transpose(1, 2).contiguous().view(batch_size, -1, self.d_model)
        return self.out_linear(context)
```

## 步骤 5：残差连接和层归一化

### 什么是残差连接和层归一化？

残差连接将子层的输入添加到其输出，层归一化对每个位置的特征进行归一化。

### 为什么需要残差连接？

残差连接通过 ( x + \text{sub-layer}(x) ) 允许梯度直接流动，缓解深层网络的梯度消失问题。

### 为什么需要层归一化？

层归一化通过标准化每个位置的特征（均值为 0，方差为 1），减少内部协变量偏移，稳定训练。

**示例：**
对于“我爱深度学习”的嵌入向量，经过多头注意力后，残差连接将输入嵌入与注意力输出相加，然后归一化。

**代码示例：**

```python
class TransformerBlock(nn.Module):
    def __init__(self, d_model, num_heads):
        super().__init__()
        self.mha = MultiHeadAttention(d_model, num_heads)
        self.norm1 = nn.LayerNorm(d_model)
        self.ffn = nn.Sequential(
            nn.Linear(d_model, d_model * 4),
            nn.ReLU(),
            nn.Linear(d_model * 4, d_model)
        )
        self.norm2 = nn.LayerNorm(d_model)
        self.dropout = nn.Dropout(0.1)

    def forward(self, x, mask=None):
        attn_output = self.mha(x, x, x, mask)
        x = self.norm1(x + self.dropout(attn_output))
        ffn_output = self.ffn(x)
        x = self.norm2(x + self.dropout(ffn_output))
        return x
```

## 步骤 6：前馈神经网络

### 什么是前馈神经网络？

前馈神经网络是对每个位置独立应用的全连接网络，通常包含两个线性层和 ReLU 激活。

### 为什么需要前馈神经网络？

它为模型引入非线性，允许学习更复杂的模式。每个位置独立处理，确保计算效率。

**公式：**
[ \text{FFN}(x) = \max(0, x W_1 + b_1) W_2 + b_2 ]

**示例：**
对于“我”的嵌入向量 [0.576, 0.141, 0.141, 0.141]，FFN 可能将其扩展到更高维度（如 2048），应用 ReLU，然后投影回 512 维。

## 步骤 7：重复 Transformer 块

### 为什么需要重复 Transformer 块？

堆叠多个 Transformer 块（如 6 层）增加模型深度，允许学习更复杂的模式。每一层进一步精炼表示。

**示例：**
对于“我爱深度学习”，6 层 Transformer 块依次处理嵌入向量，逐步捕捉更高级的语义关系。

## 步骤 8：输出概率

### 什么是输出概率？

通过线性层将 Transformer 块的输出投影到词汇表大小，然后通过 softmax 生成每个 token 的概率分布。

### 为什么需要输出概率？

输出概率用于预测下一个 token，例如在语言建模中，模型预测“我爱深度学习”后的下一个词。

**示例：**
假设词汇表大小为 10,000，输出为 (4, 10,000) 的概率分布，表示每个 token 的下一个 token 概率。

**代码示例：**

```python
class LanguageModel(nn.Module):
    def __init__(self, vocab_size, d_model, num_heads, num_layers):
        super().__init__()
        self.embedding = nn.Embedding(vocab_size, d_model)
        self.pos_encoder = PositionalEncoding(d_model)
        self.transformer = nn.ModuleList([TransformerBlock(d_model, num_heads) for _ in range(num_layers)])
        self.fc = nn.Linear(d_model, vocab_size)

    def forward(self, x, mask=None):
        x = self.embedding(x)
        x = self.pos_encoder(x)
        for block in self.transformer:
            x = block(x, mask)
        return self.fc(x)
```

## 训练 Transformer

### 训练过程

Transformer 训练通常分为预训练和微调：

- **预训练**：在大型语料库上使用因果语言建模（预测下一个 token）训练模型，学习通用语言表示。
- **微调**：在特定任务（如翻译、问答）上调整模型参数。

**损失函数**：交叉熵损失，比较预测概率与真实 token。

**优化器**：通常使用 Adam 优化器，结合学习率调度（如 warmup）。

**示例：**
对于“我爱深度学习”，模型预测“学习”后的下一个 token，计算预测概率与真实 token 的损失，调整权重。

## 与编码器-解码器 Transformer 的比较

解码器-仅 Transformer（如 GPT）用于生成任务，关注自身输入。编码器-解码器 Transformer（如原始 Transformer）适合翻译：

- **编码器**：处理输入序列，生成表示。
- **解码器**：生成输出序列，关注编码器输出和自身输入。

## 常见问题解答（FAQ）

1. **为什么 Transformer 不使用循环结构？**
   Transformer 通过自注意力并行处理整个序列，避免 RNN 的顺序处理瓶颈，提高效率。
2. **如何处理长距离依赖？**
   自注意力允许每个 token 关注所有其他 token，无论距离远近。
3. **多头注意力的作用是什么？**
   多头机制允许模型同时关注序列的不同方面，如语法和语义。
4. **为什么需要位置编码？**
   自注意力不考虑顺序，位置编码提供 token 位置信息。
5. **Transformer 如何处理长序列？**
   标准 Transformer 的计算复杂度为 ( O(n^2) )，长序列可能导致内存问题。高效变体（如 Longformer）通过稀疏注意力解决。

## 高级话题与未来方向

截至 2025 年，Transformer 技术持续发展：

- **高效 Transformer**：如 [Longformer](https://arxiv.org/abs/2004.05150)，通过稀疏注意力处理长序列。
- **多模态 Transformer**：如 [Vision Transformer](https://arxiv.org/abs/2010.11929)，将图像分块视为 token。
- **稀疏 Transformer**：减少计算量。
- **专家混合 (MoE)**：如 [Switch Transformer](https://arxiv.org/abs/2101.03961)，提升可扩展性。
- **大型语言模型**：如 GPT-3，参数达 1750 亿。

未来研究可能带来更高效或替代架构，但 Transformer 仍是 AI 核心。

## 结论

本教程通过详细解释、代码示例和图示，为初学者提供了 Transformer 架构的全面介绍。通过示例句子“我爱深度学习”，我们展示了每个步骤的作用和必要性。希望您通过本教程对 Transformer 有深入理解，并为进一步学习打下基础。

## 参考文献

- [Attention is All You Need - Transformer Paper](https://arxiv.org/abs/1706.03762)
- [The Illustrated Transformer - Visual Guide](https://jalammar.github.io/illustrated-transformer/)
- [Harvard NLP Transformer Tutorials](https://scholar.harvard.edu/binxuw/classes/machine-learning-scratch/materials/transformers)
- [Longformer: Efficient Transformer for Long Sequences](https://arxiv.org/abs/2004.05150)
- [Vision Transformer: Transformers for Images](https://arxiv.org/abs/2010.11929)
- [Switch Transformer: Scalable Model with MoE](https://arxiv.org/abs/2101.03961)
